{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Exploratory Data Analysis (EDA)\n",
                "\n",
                "## Objective\n",
                "As a first step in our data science lifecycle, we need to understand the data we are working with. This notebook is strictly for **analysis and understanding**. We are not training models or building production pipelines here.\n",
                "\n",
                "**Goals:**\n",
                "1. Understand the data structure (rows, columns, types).\n",
                "2. Check for class imbalance (how many toxic vs non-toxic?).\n",
                "3. Analyze the text data (length, common patterns).\n",
                "4. visualize the samples to understand the problem 'humanly'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Set plot style for better aesthetics\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "We load the raw training data. Note that we access it from the `../data/raw/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = '../data/raw/train.csv'\n",
                "\n",
                "if os.path.exists(DATA_PATH):\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    print(\"Data loaded successfully.\")\n",
                "else:\n",
                "    print(f\"File not found at {DATA_PATH}. Please ensure data is placed in the data/raw folder.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Basic Inspection\n",
                "Let's look at the shape of the dataset and the data types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals():\n",
                "    print(f\"Dataset Shape: {df.shape}\")\n",
                "    print(\"\\nFirst 5 rows:\")\n",
                "    display(df.head())\n",
                "    print(\"\\nData Info:\")\n",
                "    df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Target Distribution (Class Imbalance)\n",
                "In toxicity detection, we expect most comments to be normal (safe). Let's quantify this imbalance.\n",
                "We assume the target column is `toxic`. If the dataset has multiple labels (like the Jigsaw dataset), we will focus on the main `toxic` flag for this beginner project."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_col = 'toxic'\n",
                "\n",
                "if 'df' in locals() and target_col in df.columns:\n",
                "    counts = df[target_col].value_counts()\n",
                "    percentage = df[target_col].value_counts(normalize=True) * 100\n",
                "    \n",
                "    print(\"Class Counts:\")\n",
                "    print(counts)\n",
                "    print(\"\\nClass Percentage:\")\n",
                "    print(percentage)\n",
                "    \n",
                "    plt.figure(figsize=(8, 5))\n",
                "    sns.countplot(x=target_col, data=df)\n",
                "    plt.title('Distribution of Toxic (1) vs Non-Toxic (0) Comments')\n",
                "    plt.xlabel('Is Toxic?')\n",
                "    plt.ylabel('Count')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation:**\n",
                "If the '0' bar is much higher than the '1' bar, we have a class imbalance. This means our model might become biased towards predicting 'Safe'. We will need to keep this in mind during metric selection (Accuracy might be misleading; F1-score is better)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Text Length Analysis\n",
                "Do toxic comments differ in length from normal comments? Let's check character counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals():\n",
                "    # Create a temporary column for length analysis\n",
                "    df['text_length'] = df['comment_text'].astype(str).str.len()\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Histogram\n",
                "    plt.subplot(1, 2, 1)\n",
                "    sns.histplot(data=df, x='text_length', hue=target_col, bins=50, kde=True, log_scale=True)\n",
                "    plt.title('Text Length Distribution (Log Scale)')\n",
                "    \n",
                "    # Boxplot\n",
                "    plt.subplot(1, 2, 2)\n",
                "    sns.boxplot(x=target_col, y='text_length', data=df)\n",
                "    plt.yscale('log')\n",
                "    plt.title('Text Length by Class')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sample Inspection\n",
                "Numbers are great, but reading the data is crucial in NLP. Warning: Content may be offensive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals() and target_col in df.columns:\n",
                "    print(\"--- SAMPLE TOXIC COMMENTS ---\")\n",
                "    toxic_samples = df[df[target_col] == 1]['comment_text'].sample(5, random_state=42).values\n",
                "    for i, text in enumerate(toxic_samples):\n",
                "        print(f\"{i+1}. {text[:200]}...\") # Truncating for display\n",
                "        print(\"-\"*50)\n",
                "        \n",
                "    print(\"\\n--- SAMPLE SAFE COMMENTS ---\")\n",
                "    safe_samples = df[df[target_col] == 0]['comment_text'].sample(5, random_state=42).values\n",
                "    for i, text in enumerate(safe_samples):\n",
                "        print(f\"{i+1}. {text[:200]}...\")\n",
                "        print(\"-\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Multi-Label Overview (Optional)\n",
                "If the dataset has granular labels, let's see how they overlap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "additional_labels = ['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
                "present_labels = [col for col in additional_labels if col in df.columns]\n",
                "\n",
                "if present_labels:\n",
                "    print(f\"Found granular labels: {present_labels}\")\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.heatmap(df[[target_col] + present_labels].corr(), annot=True, cmap='coolwarm', vmin=0, vmax=1)\n",
                "    plt.title('Correlation between Toxicity Types')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary & Next Steps\n",
                "\n",
                "1. **Data Loaded**: We have confirmed the data exists and is readable.\n",
                "2. **Imbalance**: We observed the ratio of toxic comments. We shouldn't use accuracy alone as a metric.\n",
                "3. **Text**: We have an idea of how long the comments are, which will help us decide the `MAX_LEN` for our Deep Learning model.\n",
                "\n",
                "**Next Phase:** Model Experimentation (`02_model_experimentation.ipynb`). We will try to build a baseline model to see if we can detect these patterns."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}