{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Model Experimentation & Selection\n",
                "\n",
                "## Objective\n",
                "In this notebook, we will experiment with different approaches to solve the Toxicity Detection problem. We will start with a simple baseline and then move to a Deep Learning solution. The goal is to decide the final architecture and preprocessing steps for our production code.\n",
                "\n",
                "**Experiments:**\n",
                "1. **Baseline**: TF-IDF Vectorization + Logistic Regression.\n",
                "2. **Deep Learning**: LSTM (Long Short-Term Memory) with Keras.\n",
                "\n",
                "**Outcome:**\n",
                "At the end of this notebook, we will clearly state the configurations (Max Length, Vocab Size, Model Arch) that will be moved to `src/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
                "\n",
                "import os\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load raw data\n",
                "DATA_PATH = '../data/raw/train.csv'\n",
                "\n",
                "if os.path.exists(DATA_PATH):\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    # Minimal cleaning for experiment\n",
                "    df['comment_text'] = df['comment_text'].fillna('')\n",
                "    print(f\"Loaded {len(df)} rows.\")\n",
                "else:\n",
                "    print(\"Error: Data not found.\")\n",
                "\n",
                "# Target variable\n",
                "target_col = 'toxic'\n",
                "\n",
                "# Sample for faster experimentation (Optional - comment out for full run)\n",
                "# df = df.sample(20000, random_state=42)\n",
                "\n",
                "X = df['comment_text'].astype(str)\n",
                "y = df[target_col]\n",
                "\n",
                "# Split: 80% Train, 20% Test\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Train shape: {X_train.shape}\")\n",
                "print(f\"Test shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline Model: TF-IDF + Logistic Regression\n",
                "Always start simple. If a simple model gives 95% accuracy, complex deep learning might be overkill."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectorize\n",
                "vectorizer = TfidfVectorizer(max_features=5000)\n",
                "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
                "X_test_tfidf = vectorizer.transform(X_test)\n",
                "\n",
                "# Train\n",
                "lr_model = LogisticRegression(max_iter=1000)\n",
                "lr_model.fit(X_train_tfidf, y_train)\n",
                "\n",
                "# Evaluate\n",
                "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
                "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
                "print(\"\\nClassification Report (Baseline):\\n\")\n",
                "print(classification_report(y_test, y_pred_lr))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Deep Learning Experiment: LSTM\n",
                "Now we try a specialized sequence model. LSTMs are great for understanding the context in text data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration (These are the Hyperparameters we define for production)\n",
                "MAX_VOCAB_SIZE = 20000  # Max unique words to keep\n",
                "MAX_LEN = 150           # Max length of a comment (based on EDA)\n",
                "EMBEDDING_DIM = 64      # Size of word vectors\n",
                "\n",
                "# 1. Tokenization\n",
                "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
                "tokenizer.fit_on_texts(X_train)\n",
                "\n",
                "# 2. Convert text to sequences\n",
                "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
                "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
                "\n",
                "# 3. Padding (make all sequences same length)\n",
                "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
                "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
                "\n",
                "print(f\"Data prepared. X_train_pad shape: {X_train_pad.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the Model Architecture\n",
                "model = Sequential([\n",
                "    Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN),\n",
                "    Bidirectional(LSTM(64, return_sequences=True)), # Bidirectional learns context from both directions\n",
                "    tf.keras.layers.GlobalMaxPool1D(),              # Reduces dimensionality\n",
                "    Dense(64, activation='relu'),\n",
                "    Dropout(0.3),                                   # Prevents overfitting\n",
                "    Dense(1, activation='sigmoid')                  # Binary classification output\n",
                "])\n",
                "\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train (Using fewer epochs for experiment)\n",
                "# In production, we might use EarlyStopping and more epochs\n",
                "history = model.fit(\n",
                "    X_train_pad, y_train,\n",
                "    batch_size=32,\n",
                "    epochs=2,  # Keep it short for notebook experiment\n",
                "    validation_split=0.1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate LSTM\n",
                "y_pred_probs = model.predict(X_test_pad)\n",
                "y_pred_lstm = (y_pred_probs > 0.5).astype(int)\n",
                "\n",
                "print(\"LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
                "print(\"\\nClassification Report (LSTM):\\n\")\n",
                "print(classification_report(y_test, y_pred_lstm))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Decision & Conclusion\n",
                "\n",
                "**Comparison:**\n",
                "- Check F1-scores of class '1' (Toxic) for both models.\n",
                "- Deep Learning usually outperforms on larger datasets or more complex language structures.\n",
                "\n",
                "**Production Plan:**\n",
                "Based on these experiments, we will adopt the **LSTM approach** for our production codebase.\n",
                "\n",
                "**Parameters for `src/`:**\n",
                "- `MAX_WORDS` (Vocab) = 20,000\n",
                "- `MAX_LEN` = 150\n",
                "- `EMBEDDING_DIM` = 64\n",
                "- Architecture: Embedding -> Bi-LSTM -> GlobalMaxPool -> Dense -> Dropout -> Output"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}